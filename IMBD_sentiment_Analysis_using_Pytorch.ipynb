{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMBD sentiment Analysis using Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTZ6pK4VhdSQ"
      },
      "source": [
        "# **Dataset Details - IMDB Dataset of 50K Movie Reviews**\n",
        "IMDB dataset having 50K movie reviews for natural language processing or Text analytics.\n",
        "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. A set of 25,000 highly polar movie reviews for training and 25,000 for testing is provided. \n",
        "\n",
        "**Goal -** To predict the number of positive and negative reviews using either classification or deep learning algorithms.\n",
        "\n",
        "**Link to Dataset :** https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwS8_7GSjPkQ"
      },
      "source": [
        "# **Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4qnpw__mNQi"
      },
      "source": [
        "**Load In and Visualize Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBa1f9Ft6lHi",
        "outputId": "3388becc-c333-45fe-e59b-de8194d23a75"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive') # mount your drive using drive.mount()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "lM1ionEphRIT",
        "outputId": "11a0fd25-3e6a-467e-aa50-c6cce6373be4"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# access file from your Google Drive directly\n",
        "with open('/content/drive/My Drive/Advanced Machine Learning Project/IMDB Dataset.csv', 'r') as f: \n",
        "  df = pd.read_csv(f)\n",
        "df.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cNrPMM3mv6g"
      },
      "source": [
        "**Convert Data to Lower Case**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "LA2FWoQem1gG",
        "outputId": "a83e3d1c-6667-4297-bacb-07b6f40bdc55"
      },
      "source": [
        "df['review'] = df['review'].apply(lambda x:x.lower()) # lambda functions help to perform small tasks with less code\n",
        "df['review'][0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"one of the other reviewers has mentioned that after watching just 1 oz episode you'll be hooked. they are right, as this is exactly what happened with me.<br /><br />the first thing that struck me about oz was its brutality and unflinching scenes of violence, which set in right from the word go. trust me, this is not a show for the faint hearted or timid. this show pulls no punches with regards to drugs, sex or violence. its is hardcore, in the classic use of the word.<br /><br />it is called oz as that is the nickname given to the oswald maximum security state penitentary. it focuses mainly on emerald city, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. em city is home to many..aryans, muslims, gangstas, latinos, christians, italians, irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />i would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. forget pretty pictures painted for mainstream audiences, forget charm, forget romance...oz doesn't mess around. the first episode i ever saw struck me as so nasty it was surreal, i couldn't say i was ready for it, but as i watched more, i developed a taste for oz, and got accustomed to the high levels of graphic violence. not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) watching oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdKsK6Aym-ym"
      },
      "source": [
        "**Remove Punctuation from Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHIsiO5nnH47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "199fd3fe-13b1-485d-91c3-9725bfee5c26"
      },
      "source": [
        "from string import punctuation\n",
        "print(punctuation)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLpMxllSnbIU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "eba818f1-78cd-4c32-b598-9ea8d808e858"
      },
      "source": [
        "df['clean_text'] = df['review'].apply(lambda x:''.join([c for c in x if c not in punctuation]))\n",
        "df['clean_text'][0]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'one of the other reviewers has mentioned that after watching just 1 oz episode youll be hooked they are right as this is exactly what happened with mebr br the first thing that struck me about oz was its brutality and unflinching scenes of violence which set in right from the word go trust me this is not a show for the faint hearted or timid this show pulls no punches with regards to drugs sex or violence its is hardcore in the classic use of the wordbr br it is called oz as that is the nickname given to the oswald maximum security state penitentary it focuses mainly on emerald city an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda em city is home to manyaryans muslims gangstas latinos christians italians irish and moreso scuffles death stares dodgy dealings and shady agreements are never far awaybr br i would say the main appeal of the show is due to the fact that it goes where other shows wouldnt dare forget pretty pictures painted for mainstream audiences forget charm forget romanceoz doesnt mess around the first episode i ever saw struck me as so nasty it was surreal i couldnt say i was ready for it but as i watched more i developed a taste for oz and got accustomed to the high levels of graphic violence not just violence but injustice crooked guards wholl be sold out for a nickel inmates wholl kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience watching oz you may become comfortable with what is uncomfortable viewingthats if you can get in touch with your darker side'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVy-xB55nUPW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "89977644-1891-4adc-c30f-58162dea68df"
      },
      "source": [
        "df['len_review'] = df['clean_text'].apply(lambda x:len(x))\n",
        "df.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>len_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>one of the other reviewers has mentioned that ...</td>\n",
              "      <td>1683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
              "      <td>positive</td>\n",
              "      <td>a wonderful little production br br the filmin...</td>\n",
              "      <td>954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "      <td>i thought this was a wonderful way to spend ti...</td>\n",
              "      <td>886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>basically theres a family where a little boy j...</td>\n",
              "      <td>706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
              "      <td>positive</td>\n",
              "      <td>petter matteis love in the time of money is a ...</td>\n",
              "      <td>1261</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  ... len_review\n",
              "0  one of the other reviewers has mentioned that ...  ...       1683\n",
              "1  a wonderful little production. <br /><br />the...  ...        954\n",
              "2  i thought this was a wonderful way to spend ti...  ...        886\n",
              "3  basically there's a family where a little boy ...  ...        706\n",
              "4  petter mattei's \"love in the time of money\" is...  ...       1261\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCFA1tiXocLR"
      },
      "source": [
        "**Tokenize and Create a list of reviews**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEv3lNE1omlQ"
      },
      "source": [
        "all_text2 = df['clean_text'].tolist()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwSwiQT3pifw"
      },
      "source": [
        "Create Vocab to Int mapping dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9YfhcVCpjUD"
      },
      "source": [
        "from collections import Counter\n",
        "all_text2 = ' '.join(all_text2)\n",
        "\n",
        "# create a list of words\n",
        "words = all_text2.split()\n",
        "\n",
        "# Count all the words using Counter Method\n",
        "count_words = Counter(words)\n",
        "\n",
        "total_words = len(words)\n",
        "sorted_words = count_words.most_common(total_words)\n",
        "\n",
        "vocab_to_int = {w:i for i, (w,c) in enumerate(sorted_words)}\n",
        "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
        "reviews_split = df['clean_text'].tolist()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyVuy4rOqHjS"
      },
      "source": [
        "Encode the Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpFgpCu9qN7z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "65330d65-4043-4616-f1d4-dfaad78961f7"
      },
      "source": [
        "reviews_int = []\n",
        "for review in reviews_split:\n",
        "    r = [vocab_to_int[w] for w in review.split()]\n",
        "    reviews_int.append(r)\n",
        "print (reviews_int[0:3])\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[28, 4, 1, 77, 1941, 44, 1063, 11, 100, 145, 40, 479, 3324, 393, 461, 26, 3190, 34, 23, 205, 14, 10, 6, 601, 48, 590, 15, 2137, 12, 1, 87, 146, 11, 3255, 69, 42, 3324, 13, 29, 5600, 2, 15378, 134, 4, 582, 61, 282, 7, 205, 35, 1, 670, 138, 1707, 69, 10, 6, 21, 3, 118, 16, 1, 8330, 5794, 39, 11861, 10, 118, 2508, 55, 6064, 15, 5636, 5, 1470, 381, 39, 582, 29, 6, 3407, 7, 1, 352, 340, 4, 1, 23503, 12, 8, 6, 469, 3324, 14, 11, 6, 1, 11516, 338, 5, 1, 16023, 6870, 2543, 1061, 61649, 8, 2637, 1375, 20, 25365, 536, 33, 4727, 2520, 4, 1, 1208, 112, 31, 1, 7153, 25, 2992, 13015, 2, 408, 61650, 37, 17529, 6, 21, 319, 20, 1, 5098, 3720, 536, 6, 344, 5, 81744, 8470, 41120, 15379, 5171, 7893, 2461, 2, 18404, 61651, 329, 9265, 7472, 13445, 2, 8721, 34936, 23, 109, 224, 5436, 12, 9, 57, 128, 1, 269, 1303, 4, 1, 118, 6, 668, 5, 1, 187, 11, 8, 262, 112, 77, 257, 548, 3001, 819, 178, 1271, 4349, 16, 2499, 1096, 819, 1412, 819, 81745, 148, 978, 181, 1, 87, 393, 9, 120, 201, 3255, 69, 14, 37, 1574, 8, 13, 2214, 9, 397, 128, 9, 13, 1550, 16, 8, 18, 14, 9, 278, 51, 9, 1463, 3, 1250, 16, 3324, 2, 183, 10277, 5, 1, 319, 2092, 4, 2100, 582, 21, 40, 582, 18, 7965, 7154, 4974, 14178, 26, 2970, 45, 16, 3, 32611, 7035, 14178, 494, 20, 620, 2, 75, 240, 15, 8, 73, 9934, 753, 816, 7035, 106, 660, 81, 1208, 20591, 668, 5, 63, 549, 4, 931, 1996, 39, 1208, 558, 145, 3324, 22, 196, 411, 3778, 15, 48, 6, 3275, 81746, 43, 22, 68, 75, 7, 1211, 15, 122, 4018, 501], [3, 384, 115, 358, 12, 12, 1, 1365, 3015, 6, 52, 18405, 52, 81747, 1628, 2, 391, 3, 13446, 2, 518, 27652, 280, 4, 1881, 5, 1, 421, 407, 12, 12, 1, 150, 23, 546, 73, 2274, 492, 4607, 21, 60, 44, 183, 31, 1, 81748, 18, 27, 44, 31, 1, 2222, 184, 3295, 99, 22, 68, 349, 64, 1, 13906, 797, 10150, 32, 1, 1794, 5, 1664, 7473, 6746, 21, 60, 6, 8, 73, 266, 1, 145, 18, 8, 6, 3, 41121, 427, 2, 2368, 407, 3, 4350, 358, 42, 28, 4, 1, 79, 3208, 4, 213, 2, 24, 119, 12, 12, 1, 1881, 62, 260, 344, 15, 1, 115, 177, 1, 1069, 4, 1, 2983, 61, 241, 70, 340, 1, 2140, 1024, 3160, 1243, 1149, 90, 5052, 8, 288, 20, 249, 1796, 2, 249, 4608, 567, 15, 1, 134, 3605, 19962, 2, 27653, 2, 1, 713, 567, 4, 63, 1094, 15, 81749, 61652, 29081, 166, 2259, 23, 1900, 73, 223], [9, 194, 10, 13, 3, 384, 98, 5, 1110, 59, 20, 3, 99, 905, 1466, 2484, 1192, 7, 1, 946, 16739, 764, 2, 145, 3, 4019, 213, 1, 114, 6, 4053, 18, 1, 405, 6, 1862, 2, 1, 101, 23, 1473, 54, 1, 73, 6602, 6697, 1540, 498, 133, 46, 196, 26, 694, 50, 34, 929, 10, 6, 21, 1030, 221, 283, 2993, 5197, 9, 194, 8, 13, 3032, 11, 2934, 2033, 6, 125, 1377, 7, 1155, 4, 1, 429, 104, 4, 169, 25, 2327, 5, 6196, 12, 10, 13, 1, 86, 452, 1390, 30, 28, 4, 19963, 1294, 7, 151, 3001, 9, 128, 3, 2101, 133, 195, 109, 74, 1481, 15, 9266, 41122, 7, 10, 58, 1268, 5, 1234, 184, 41, 1237, 1393, 2, 5033, 205, 81, 3, 859, 18, 5099, 182, 11078, 12, 10, 196, 21, 26, 1, 7597, 5216, 4, 24, 642, 18, 8, 13, 32612, 70, 2285, 2877, 25366, 2, 51, 218, 70, 2638, 3, 79, 213, 5, 138, 64, 15, 333]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>len_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>one of the other reviewers has mentioned that ...</td>\n",
              "      <td>1683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
              "      <td>positive</td>\n",
              "      <td>a wonderful little production br br the filmin...</td>\n",
              "      <td>954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "      <td>i thought this was a wonderful way to spend ti...</td>\n",
              "      <td>886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>basically theres a family where a little boy j...</td>\n",
              "      <td>706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
              "      <td>positive</td>\n",
              "      <td>petter matteis love in the time of money is a ...</td>\n",
              "      <td>1261</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  ... len_review\n",
              "0  one of the other reviewers has mentioned that ...  ...       1683\n",
              "1  a wonderful little production. <br /><br />the...  ...        954\n",
              "2  i thought this was a wonderful way to spend ti...  ...        886\n",
              "3  basically there's a family where a little boy ...  ...        706\n",
              "4  petter mattei's \"love in the time of money\" is...  ...       1261\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOEPy7AFqWfX"
      },
      "source": [
        "Encode the Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTsH_45VqaGl"
      },
      "source": [
        "labels_split = df['sentiment'].tolist()\n",
        "encoded_labels = [1 if label =='positive' else 0 for label in labels_split]\n",
        "encoded_labels = np.array(encoded_labels)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey4PguRVq5PD"
      },
      "source": [
        "**Analyze Length of Reviews**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrWp5IAhq-G3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "1e8e038a-c502-4294-d82b-f690d7fbaacb"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "reviews_len = [len(x) for x in reviews_int]\n",
        "pd.Series(reviews_len).hist()\n",
        "plt.show()\n",
        "pd.Series(reviews_len).describe()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    50000.000000\n",
              "mean       230.258240\n",
              "std        170.663887\n",
              "min          4.000000\n",
              "25%        126.000000\n",
              "50%        172.000000\n",
              "75%        280.000000\n",
              "max       2469.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-saBeEQrPnF"
      },
      "source": [
        "**Removing Outliers — Getting rid of extremely long or short reviews**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH8EOAIPrXRP"
      },
      "source": [
        "reviews_int = [ reviews_int[i] for i, l in enumerate(reviews_len) if l > 0 ]\n",
        "encoded_labels = [ encoded_labels[i] for i, l in enumerate(reviews_len) if l > 0 ]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frB_LWXurcOU"
      },
      "source": [
        "**Padding / Truncating the remaining data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reBwGCybrhgc"
      },
      "source": [
        "def pad_features(reviews_int, seq_length):\n",
        "  \n",
        "    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
        "    '''\n",
        "    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n",
        "    \n",
        "    for i, review in enumerate(reviews_int):\n",
        "        review_len = len(review)\n",
        "        \n",
        "        if review_len <= seq_length:\n",
        "            zeroes = list(np.zeros(seq_length-review_len))\n",
        "            new = zeroes+review\n",
        "        elif review_len > seq_length:\n",
        "            new = review[0:seq_length]\n",
        "        \n",
        "        features[i,:] = np.array(new)\n",
        "    \n",
        "    return features"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8EJ29tOrm0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a93e6e-9569-4fde-ac0c-2dc28e77b541"
      },
      "source": [
        "features = pad_features(reviews_int,200)\n",
        "print (features[:10,:])\n",
        "\n",
        "len_feat = len(features)\n",
        "split_frac = 0.8\n",
        "print(len_feat)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   28     4     1 ...   978   181     1]\n",
            " [    0     0     0 ...  1900    73   223]\n",
            " [    0     0     0 ...    64    15   333]\n",
            " ...\n",
            " [    0     0     0 ...    20     1   946]\n",
            " [    0     0     0 ... 10151 45601    60]\n",
            " [    0     0     0 ...    12    79  1215]]\n",
            "50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_QhJ9zIsbwU"
      },
      "source": [
        "**Dataset Split - Training, Validation, Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFM5Qu51siih"
      },
      "source": [
        "split_frac = 0.8\n",
        "train_x = features[0:int(split_frac*len_feat)]\n",
        "train_y = encoded_labels[0:int(split_frac*len_feat)]\n",
        "remaining_x = features[int(split_frac*len_feat):]\n",
        "remaining_y = encoded_labels[int(split_frac*len_feat):]\n",
        "valid_x = remaining_x[0:int(len(remaining_x)*0.5)]\n",
        "valid_y = remaining_y[0:int(len(remaining_y)*0.5)]\n",
        "test_x = remaining_x[int(len(remaining_x)*0.5):]\n",
        "test_y = remaining_y[int(len(remaining_y)*0.5):]\n",
        "\n",
        "type(test_y)\n",
        "train_y = np.array(train_y)\n",
        "test_y = np.array(test_y)\n",
        "valid_y = np.array(valid_y)\n"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqz03tcUs9Fm"
      },
      "source": [
        "**Dataloaders and Batching**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWVUgNZVs-Po"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij0HA6UrtPLp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa1fc4ef-f2b4-4c31-a483-8db48ca710a4"
      },
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([50, 200])\n",
            "Sample input: \n",
            " tensor([[     0,      0,      0,  ...,   1820,     10,     28],\n",
            "        [     0,      0,      0,  ...,     44,    120,    105],\n",
            "        [  1029, 115174,      2,  ...,      2,   3152,     41],\n",
            "        ...,\n",
            "        [    39,     30,    217,  ...,     91,     11,     94],\n",
            "        [     0,      0,      0,  ...,     17,    275,  40715],\n",
            "        [     0,      0,      0,  ...,     25,    120,    105]])\n",
            "\n",
            "Sample label size:  torch.Size([50])\n",
            "Sample label: \n",
            " tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
            "        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kDK2njst191"
      },
      "source": [
        "# **Model Definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-WVA5PAt6PI"
      },
      "source": [
        "**Define the LSTM Network Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFvDTUt3t-JM"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    \n",
        "        # stack up lstm outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evwD-fwCuu9O"
      },
      "source": [
        "# **Model Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDE-RtqSu-Nc"
      },
      "source": [
        "**Instantiate the network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9AlxLE-u0Sw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "456bd2a4-27ef-482a-ba3d-bfc62a5db0c7"
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(181686, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4OmWo7FvFHn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e554e531-72d7-4e26-ea59-4a552a40b2f6"
      },
      "source": [
        "# First checking if GPU is available\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsnMbwdnvOpB"
      },
      "source": [
        "**Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQGsjCmQvzUs"
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifQ57dt5w0kG"
      },
      "source": [
        "# training params\n",
        "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOA2PCY1wItd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1794651b-05e2-4c37-b6d8-6815d40d2f1f"
      },
      "source": [
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "        #print(counter)\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/4... Step: 100... Loss: 0.620039... Val Loss: 0.697463\n",
            "Epoch: 1/4... Step: 200... Loss: 0.722154... Val Loss: 0.612525\n",
            "Epoch: 1/4... Step: 300... Loss: 0.692321... Val Loss: 0.693094\n",
            "Epoch: 1/4... Step: 400... Loss: 0.612899... Val Loss: 0.656565\n",
            "Epoch: 1/4... Step: 500... Loss: 0.481736... Val Loss: 0.558929\n",
            "Epoch: 1/4... Step: 600... Loss: 0.476891... Val Loss: 0.658610\n",
            "Epoch: 1/4... Step: 700... Loss: 0.495688... Val Loss: 0.491266\n",
            "Epoch: 1/4... Step: 800... Loss: 0.428548... Val Loss: 0.459063\n",
            "Epoch: 2/4... Step: 900... Loss: 0.428912... Val Loss: 0.426378\n",
            "Epoch: 2/4... Step: 1000... Loss: 0.344904... Val Loss: 0.408977\n",
            "Epoch: 2/4... Step: 1100... Loss: 0.313144... Val Loss: 0.422823\n",
            "Epoch: 2/4... Step: 1200... Loss: 0.371934... Val Loss: 0.390398\n",
            "Epoch: 2/4... Step: 1300... Loss: 0.305166... Val Loss: 0.374824\n",
            "Epoch: 2/4... Step: 1400... Loss: 0.246883... Val Loss: 0.353225\n",
            "Epoch: 2/4... Step: 1500... Loss: 0.239708... Val Loss: 0.365080\n",
            "Epoch: 2/4... Step: 1600... Loss: 0.339693... Val Loss: 0.362075\n",
            "Epoch: 3/4... Step: 1700... Loss: 0.326826... Val Loss: 0.448833\n",
            "Epoch: 3/4... Step: 1800... Loss: 0.235476... Val Loss: 0.372029\n",
            "Epoch: 3/4... Step: 1900... Loss: 0.220380... Val Loss: 0.356555\n",
            "Epoch: 3/4... Step: 2000... Loss: 0.190736... Val Loss: 0.364684\n",
            "Epoch: 3/4... Step: 2100... Loss: 0.169092... Val Loss: 0.338821\n",
            "Epoch: 3/4... Step: 2200... Loss: 0.189312... Val Loss: 0.357870\n",
            "Epoch: 3/4... Step: 2300... Loss: 0.205163... Val Loss: 0.380186\n",
            "Epoch: 3/4... Step: 2400... Loss: 0.316158... Val Loss: 0.355723\n",
            "Epoch: 4/4... Step: 2500... Loss: 0.093086... Val Loss: 0.371486\n",
            "Epoch: 4/4... Step: 2600... Loss: 0.240009... Val Loss: 0.435140\n",
            "Epoch: 4/4... Step: 2700... Loss: 0.111778... Val Loss: 0.393367\n",
            "Epoch: 4/4... Step: 2800... Loss: 0.243736... Val Loss: 0.454291\n",
            "Epoch: 4/4... Step: 2900... Loss: 0.192231... Val Loss: 0.412623\n",
            "Epoch: 4/4... Step: 3000... Loss: 0.034993... Val Loss: 0.413760\n",
            "Epoch: 4/4... Step: 3100... Loss: 0.089654... Val Loss: 0.366873\n",
            "Epoch: 4/4... Step: 3200... Loss: 0.063948... Val Loss: 0.393384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx46ZAf-y1A1"
      },
      "source": [
        "# **Model Evaluation after Training (Testing)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYO7Q1tXkwcV"
      },
      "source": [
        "**On User Generated Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz7BXP-FkztG"
      },
      "source": [
        "def preprocess(review, vocab_to_int):\n",
        "    review = review.lower()\n",
        "    word_list = review.split()\n",
        "    num_list = []\n",
        "    #list of reviews\n",
        "    #though it contains only one review as of now\n",
        "    reviews_int = []\n",
        "    for word in word_list:\n",
        "        if word in vocab_to_int.keys():\n",
        "            num_list.append(vocab_to_int[word])\n",
        "    reviews_int.append(num_list)\n",
        "    return reviews_int\n",
        "\n",
        "def predict(net, test_review, sequence_length=200):\n",
        "    ''' Prints out whether a give review is predicted to be \n",
        "        positive or negative in sentiment, using a trained model.\n",
        "        \n",
        "        params:\n",
        "        net - A trained net \n",
        "        test_review - a review made of normal text and punctuation\n",
        "        sequence_length - the padded length of a review\n",
        "        '''\n",
        "    #change the reviews to sequence of integers\n",
        "    int_rev = preprocess(test_review, vocab_to_int)\n",
        "    #pad the reviews as per the sequence length of the feature\n",
        "    features = pad_features(int_rev, seq_length=seq_length)\n",
        "    \n",
        "    #changing the features to PyTorch tensor\n",
        "    features = torch.from_numpy(features)\n",
        "    \n",
        "    #pass the features to the model to get prediction\n",
        "    net.eval()\n",
        "    val_h = net.init_hidden(1)\n",
        "    val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        features = features.cuda()\n",
        "\n",
        "    output, val_h = net(features, val_h)\n",
        "    \n",
        "    #rounding the output to nearest 0 or 1\n",
        "    pred = torch.round(output)\n",
        "    \n",
        "    #mapping the numeric values to postive or negative\n",
        "    output = [\"Positive\" if pred.item() == 1 else \"Negative\"]\n",
        "    \n",
        "    # print custom response based on whether test_review is pos/neg\n",
        "    print(output)"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "po4Kx5CclMH9",
        "outputId": "afe2ba40-2611-4a2a-b0f2-560c31150554"
      },
      "source": [
        "test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'\n",
        "seq_length=200\n",
        "predict(net, test_review_pos, seq_length)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Positive']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8XmYh06lXSi",
        "outputId": "fb8f1797-e4cb-431b-8e00-24791a14d74f"
      },
      "source": [
        "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'\n",
        "seq_length=200\n",
        "predict(net, test_review_neg, seq_length)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Negative']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTYAlTy_zATb"
      },
      "source": [
        "**On Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWpSq96vy8km"
      },
      "source": [
        "# Get test data loss and accuracy\n",
        "\n",
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# init hidden state\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "actual = []\n",
        "predicted = []\n",
        "# iterate over test data\n",
        "for inputs, labels in test_loader:\n",
        "    \n",
        "    # Creating new variables for the hidden state, otherwise\n",
        "    # we'd backprop through the entire training history\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        actual = np.append(actual, labels.cpu().detach().numpy())\n",
        "    \n",
        "    # get predicted outputs\n",
        "    output, h = net(inputs, h)\n",
        "    \n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "    predicted = np.append(predicted, pred.cpu().detach().numpy())\n",
        "    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysyDxP2n8Srj"
      },
      "source": [
        "# **Model Qualitative Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0Mm5YSz75go",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48d252dd-aec6-480f-fa80-8dd65a207ea7"
      },
      "source": [
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.415\n",
            "Test accuracy: 0.855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqcNTafyGdMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf87d473-7fbc-42b9-e2c9-7e3adc84b8c9"
      },
      "source": [
        "# confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "matrix = confusion_matrix(actual, predicted, labels=[1,0])\n",
        "print('Confusion matrix : \\n',matrix)\n",
        "\n",
        "# outcome values order in sklearn\n",
        "tp, fn, fp, tn = confusion_matrix(actual, predicted,labels=[1,0]).reshape(-1)\n",
        "print('Outcome values : \\n', tp, fn, fp, tn)\n",
        "\n",
        "# classification report for precision, recall f1-score and accuracy\n",
        "matrix = classification_report(actual,predicted,labels=[1,0])\n",
        "print('Classification report : \\n',matrix)\n"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix : \n",
            " [[2193  277]\n",
            " [ 449 2081]]\n",
            "Outcome values : \n",
            " 2193 277 449 2081\n",
            "Classification report : \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.83      0.89      0.86      2470\n",
            "           0       0.88      0.82      0.85      2530\n",
            "\n",
            "    accuracy                           0.85      5000\n",
            "   macro avg       0.86      0.86      0.85      5000\n",
            "weighted avg       0.86      0.85      0.85      5000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW6rlx2H9J9S"
      },
      "source": [
        "# **Observations and Comments**\n",
        "\n",
        "From the **Confusion Matrix**, we get to know that:\n",
        "\n",
        "Positive Reviews correctly predicted as Positive by our model : True Positive - 2193 out of 5000\n",
        "\n",
        "Negative Reviews incorrectly predicted as Positive by our model : False Positive - 449 out of 5000\n",
        "\n",
        "Negative Reviews correctly predicted as Negative by our model : True Negative - 2081 out of 5000\n",
        "\n",
        "Positive Reviews incorrectly predicted as Negative by our model : False Neagtive - 277 out of 5000\n",
        "\n",
        "**Precision for Positive Reviews - 83%**\n",
        "*(how many of the correctly predicted cases actually turned out to be positive)*\n",
        "\n",
        "**Precision for Negative Reviews - 88%**\n",
        "*(how many of the correctly predicted cases actually turned out to be negative)*\n",
        "\n",
        "**Recall for Positive Reviews - 89%**\n",
        "*(Recall tells us how many of the actual positive cases we were able to predict correctly with our model)*\n",
        "\n",
        "**Recall for Neagtive Reviews - 82%**\n",
        "*(Recall tells us how many of the actual negative cases we were able to predict correctly with our model)*\n",
        "\n",
        "**The overall accuracy of our model is 85% which is a pretty good one for classification.**"
      ]
    }
  ]
}